# interview

## 计算机基础

### 大端小端

### 进程/线程/协程

进程：资源分配的基本单位
线程：CPU调度的基本单位

进程如果挂了，那么是不会影响到别的进程的，而线程如果挂了，那么这个进程也挂了

在Linux中进程和线程都用`task_struct`来表示，唯一的区别就是进程的mm(指向的内存区域)和files(输入流/输出流)是不同的，而线程是相同的

协程：Kotlin里是一种线程框架，本质还是依赖于线程池的，Go里是线程下的更小单位，由GO自己管理

### 进程状态

- 创建：进程在申请资源，
- 就绪：进程已经分配到了除CPU外的所有资源
- 运行：进程在执行
- 阻塞：正在执行的进程因某些事件例如IO请求而暂时无法执行，进程收到阻塞，在满足请求后就进入就绪状态
- 终止：进程结束/发送错误

### 进程/线程通信方式

- 管道：相当于一个中间介质连接了一个进程的读和另一个进程的写
- 消息队列：由一个进程向另一个进程发送消息，但存在每次都需要拷贝到进程内存的过程
- 共享内存：两个进程虚拟地址映射到同一块物理地址上
- 信号量：类似Java中Semaphore，和共享内存配合使用来控制共享内存并发访问问题
- Socket：套接字通信

### 堆和栈

堆：按需申请、动态分配，当程序申请后会从堆中找出一块可用内存标记空间占用等信息后将起始地址返回，程序结束后操作系统不会删除申请的内存，只有程序自己delete，因此会造成内存泄漏
栈：程序运行时自动拥有的一小块内存，大小在编译期时由编译器参数决定，用户保存局部变量和函数调用栈，当超出大小后会导致栈溢出

### IO模型 

![IO模型](./pics/IO.png)

阻塞IO：同步阻塞，使用系统调用，并一直阻塞直到内核将数据准备好，之后再由内核缓冲区复制到用户态，在等待内核准备的这段时间什么也干不了
非阻塞IO：同步非阻塞，内核在没有准备好数据的时候会返回错误码，而调用程序不会休眠，而是不断轮询询问内核数据是否准备好
IO多路复用：同步阻塞，类似与非阻塞，只不过轮询不是由用户线程去执行，而是由内核去轮询，内核监听程序监听到数据准备好后，调用内核函数复制数据到用户态，但可以同时监听多个文件信息
信号驱动式IO：内核在数据准备就绪时通过信号通知
异步IO：异步非阻塞，等待数据和读取数据都由内核完成了

[参考](https://www.cnblogs.com/crazymakercircle/p/10225159.html)
[参考](https://juejin.im/post/5bd32b84f265da0ac962e7c9)

### select/poll/epoll

select/poll/epoll都是IO多路复用的具体实现，好处就是单个process就可以同时处理多个网络连接的IO

select：

首先会维护一个1024的bitmap来标记FD，然后将这个bitmap拷贝到内核空间，然后系统发现置位的数据来了，那么就会在bitmap中标记并返回(用户调用select阻塞直到内核返回)，接着用户for循环O(n)找出系统说的来数据的FD，读取，然后重新标记一次bitmap，再拷贝到内核空间，周而复始

select存在的问题：

- bitmap最大1024
- 每次拷贝到内核空间的bitmap不可重用，返回后要新建一个再拷贝
- 有每次拷贝到内核空间的消耗
- O(n)次遍历，每次select返回后都需要for循环所有FD

poll：

相较于select，poll有了自己的一个数据结构pollfd，其中fd表示fd对象，events表示这个fd是要读还是写，revent表示系统这里数据的情况，每次内核返回后，在for循环遍历O(n)后重置一下revent即可

poll解决了select中1，2问题，但3，4问题依然存在

epoll：

epoll是基于事件的，通过epoll_ctl()来注册文件描述符，并通过一次拷贝到内核空间中的红黑树上，一旦文件描述符中有事件发现，内核就会将它插入到就绪列表中，当用户调用epoll_wait就会通知到

### 虚拟内存/物理内存

对虚拟内存和物理内存都进行了分页，地址中一部分是页内的偏移量，另外的部分是页号，通过进程维护的页表来记录虚拟内存地址和物理内存地址的映射关系

当出现虚拟内存大于物理内存时，其实就是虚拟地址在页表中映射到了disk硬盘上，这时会发生缺页中断，此时切换到内核态，内核从磁盘中找到数据加载到内存中，并将物理地址重新填写到页表中，再重新寻址

分页的方式使不同的进程间的内存进行了隔离，也减少内存碎片的问题

在CPU缓存中维护了个TLB快表，当查询虚拟内存映射前会先查快表，没有再去内存中查页表，从而加快速度

![内存段](./pics/memory.png)

### 用户态/内核态

在CPU的所有指令中有一些指令是很危险的，比如清内存等，因此不能允许所有程序都能使用这些指令

运行于用户态的进程可以执行的操作和访问的资源都会受到极大的限制，而运行在内核态的进程则可以执行任何操作并且在资源的使用上没有限制

大部分程序开始运行时处在用户态，但在执行过程中需要执行一些内核权限下的指令，比如开辟内存，这就涉及到了用户态到内核态的切换

发生用户态到内核态切换的三种情况：

1. 系统调用：用户态进程申请执行操作系统提供的指令
2. 异常事件：在用户态下发生了事先不可知的异常，这就会触发由当前运行的进程切换到处理该异常的内核相关程序中
3. 外设中断：当外围设备完成用户的请求操作后，会像CPU发出中断信号，此时CPU就会暂停执行下一条即将要执行的指令，转而去执行中断信号对应的处理程序，如果先前执行的指令是在用户态下，则自然就发生从用户态到内核态的转换

## 网络

### 跨域

协议，域名，端口都相同就是同源，而不同源会导致Cookie无法读取，Ajax请求被浏览器拦掉

CROS 跨域资源共享

- 简单请求：会在请求头加Origin字段，服务端响应Access-Control-Allow-Origin
- 非简单请求：在正式请求前有个预检请求，OPTION请求来询问服务器该网页是否在许可名单中

## CSRF

CSRF 跨站请求伪造，在第三方网站向被攻击网站发起请求，利用Cookie绕过验证

- 设置Cookie为同站Cookie
- 同源校验
- 在DOM树中生成个随机加密Token，请求时带上且Token每次刷新

### OSI7层模型 TCP/IP四层模型

OSI7层模型：物理层/数据链路层/网络层/传输层/会话层/表示层/应用层

![7层模型](./pics/osi7models.png)

TCP/IP四层模型：链路层/网络层/传输层/应用层

### TCP/UDP

UDP：用户数据包协议 UDP是无连接的，不能保证可靠的交付数据，面向报文传输的协议

UDP将应用层的数据直接加上UDP的首部信息发送出去

UDP没有拥塞控制

UDP首部开销小，只有8个字节

![udp首部](./pics/udp_head.png)

TCP：传输控制协议 TCP提供一种面向连接的、可靠的字节流服务

所谓面向字节流就是指TCP会将用户的数据块进行合并或分拆，然后进行传输 (TCP无界，UDP有界)

![tcp首部](./pics/tcp_head.png)

![tcp标记位](./pics/tcp_remark.png)

重要的是SYN置1表示该请求在请求连接，ACK置1表示此时请求中确认号是有效的，FIN置1表示该请求在释放连接

TCP首部20个字节，带上可选的最多60个字节

TCP的可靠传输是基于连续ARQ协议，连续ARQ协议就是一个滑动窗口(单位是字节)，滑动窗口内的内容都可以发送，当收到某个字节的确认后，方能向后滑动，如果收不到，就重传(选择重传/一起重传)

TCP流量控制：

流量控制的作用是接收方让发送方发送速率不要太快

流量控制是通过滑动窗口实现的 (TCP首部里有窗口字段，可以用来告诉发送方我这里现在可容纳的窗口大小)

当接收方告诉发送方窗口大小为0时，此时发送方会启用一个坚持定时器，来定时探测接收方的窗口大小，这是为了解决接收方传递可用的窗口大小时丢失造成死锁的情况

TCP拥塞控制：

流量控制考虑的是接收方，拥塞控制考虑的是整个网络

慢启动算法：

拥塞窗口由小到大 1，2，4，8指数级扩大，当到达慢启动阈值时，启用拥塞避免算法

拥塞避免算法：

只要网络不拥塞就试探调大拥塞窗口(每次 + 1)

一旦发现拥塞(即超时了)，那么就将慢启动阈值设为原来的一半，从1开始重新慢启动算法

快重传：

快重传要求接收方一旦接收到了个失序报文就立刻发送重复确认

快重传算法没有超时，而是收到3个以上的重复的ACK即开始重传，随后进入快恢复算法

快恢复：

在接收到3个以上的重复ACK后将慢启动阈值设为原来一半，并从一半的位置开始执行拥塞避免算法

TCP三次握手：

![三次握手](./pics/tcp_3.png)

为什么需要第三次握手，事实上第二次握手的时候客户端以及处在连接建立的状态(第三次握手SYN不等于1了)，但是存在一种客户端第一次建立连接请求在网络中过了很久才到达服务端，客户端以为超时重传了，而重传的很快服务端就回应了，而后客户端第一次的请求到了服务端，服务端不知道这个请求已经被重传过了，因此也回应了，如果此时是两次握手建立连接，那么两个连接就会被建立，而其中一个是平白无故占用服务端资源的，因此第三次握手就是来解决这种情况

TCP四次挥手：

![四次挥手](./pics/tcp_4.png)

在客户端进行最后一次挥手后会启动一个等待定时器，在等待2MSL(MSL为最长报文段寿命，通常为2分钟)，其作用是因为最后一次挥手后服务端是没有确认回复的，因此客户端需要确认服务器接收到了客户端的挥手，服务端在发起断开连接后在2MSL中如果没有收到确认，会重传，此时客户端处在等待2MSL中，因此就能回答。此外等待2MSL还能保证整个连接中的报文段都过期了，这样上一个连接中的迷失的报文段就不会影响到后一次的连接了

出现大量time_wait原因：即你是主动释放连接的一方

出现大量close_wait原因：代码中建立的连接没去close

### TCP保证可靠交付

TCP的可靠传输是基于连续ARQ协议

- 确认和重传机制 超时重传
    TCP每发送一个消息，都会设置一个定时器(超时定时器)
- 数据校验
- 数据合理分片和排序
- 流量控制
- 拥塞控制

### TCP粘包/拆包

粘包：应用程序写入的数据小于套接字缓冲区大小时，就发生粘包
拆包：应用程序写入的数据大于套接字缓冲区大小时，就发生拆包

MTU：最大传输单元，处在数据链路层，因特网中最大1500字节，当IP网络数据报长度大于MTU时就要进行分片才能传输
MSS：TCP报文中data部分的最大长度 MSS = MTU长度 - IP Header - TCP Header

### cookie/session

HTTP本身是无状态的，因此Cookie和Session是用来保存状态(比如是否登陆，用户自定义的设置等)

- Cookie保存在浏览器中，容易被获取，Session保存在服务器，相对安全
- Cookie可以设置保留时间，而Session在浏览器窗口关闭或服务器端超时都会失效
- Cookie保存数据不得超过4K，而Session没有限制

设置HttpOnly=true来使得Cookie不能改够被js获取到，只能够通过AJAX传递

当使用Session时也依赖于Cookie，当用户第一次请求服务器时，服务器会创建对应的Session，并生成SessionId返回给浏览器，而浏览器会将这个SessionID记入Cookie中，在下一次访问中浏览器会带上这个域名下的Cookie，即对应的SessionID，服务器再通过SessionID找到对应的Session，从而达到保存状态的作用

如果浏览器端禁用了Cookie，那么可以通过重写URL，如以`?sid=SessionID`的形式将SessionID拼接到URL中，或者改用Token/JWT

分布式Session解决方案：
- Nginx ip_hash 策略：通过Nginx来保证相同IP一定访问到相同的服务器
- Session复制：每个服务器上Session增删改后，会把这个Session序列化后发送给所有其他服务器上
- 共享Session：服务器无状态，将Session统一使用缓存中间件管理

### HTTP原理

HTTP协议构建于 TCP/IP 协议之上，是一个应用层协议，是计算机通过网络进行通信的规则

HTTP协议是无连接无状态的

HTTP请求报文：

报文首部(请求行/请求首部字段/通用首部字段/实体首部字段)/空行/报文主体

HTTP响应报文：

报文首部(状态行/响应首部字段/通用首部字段/实体首部字段)/空行/报文主体

报文首部：

请求行(方法，URI，HTTP版本)
请求首部字段(Host：请求资源所在服务器，Accept：可处理的媒体类型)

状态行(状态码，HTTP版本)
响应首部字段(Location：令客户端重新定向到的URI，Server：HTTP服务器的安装信息)

通用首部字段(Cache-Control：控制缓存的行为，Date：创建报文时间，Connection：连接的管理，Transfer-Encoding：报文主体的传输编码方式)
实体首部字段(Allow：资源可支持的HTTP方法，Content-Type：实体主类的类型，Content-Encoding：实体主体适用的编码方式，Content-Length：实体主体的的字节数)

一次完成HTTP请求步骤：

1. 建立TCP连接，因为HTTP是基于TCP的
2. 浏览器向服务器发送请求行(GET/URL/HTTP 1.1)
3. 浏览器向服务器发送请求头，然后发送一空白行来告诉服务器头信息发送完毕
4. 服务器向浏览器发送响应行(HTTP 1.1/200/OK)
5. 服务器向浏览器发送响应头，然后发送一空白行来告诉浏览器头信息发送完毕
6. 服务器以响应头中Content-Type描述的格式来发送实际数据
7. 服务器关闭TCP连接，除非Connection:keep-alive

### HTTP状态码

- 200 服务器成功处理了请求
- 204 服务器成功处理请求，但没有返回任何内容(CROS和RESTful的create里用到)
- 206 服务器处理了部分GET请求(HTTP1.1断点续传)

- 301 资源永久移动
- 302 资源临时移动

- 400 错误请求，请求的语法有问题
- 401 未授权，该请求要求身份验证
- 403 禁止，服务器拒绝了请求
- 404 资源未找到

- 500 服务器内部错误，无法完成请求
- 502 网关错误，服务器作为网关或代理，从上游服务器收到无效响应
- 503 服务器不可用，可能处于停机或维护状态，通常是暂时的

### HTTPS原理

非对称加密 + 对称加密 + CA证书

所谓CA证书是包含了服务商(网站)的公钥，组织信息，域名信息等，然后对这些信息进行一个数字签名(hash)，最后再将签名用CA服务商私钥加密，传到客户端这，客户端用系统内置的CA服务商的公钥解密后得到数字签名，对明文信息进行hash后和解密出的内容做对比，相符就代表正确

1. C -> S 支持的SSL版本，支持的加密算法，随机数1
2. S -> C 确认SSL版本，确认加密算法，随机数2，CA证书
3. C 校验 CA证书(本质是CA服务商的私钥加密了的S端的共钥) 此时C端就确认了S端的公钥
4. C -> S 随机数3(需要用S端的公钥进行加密) hash(第一步，第二步中的数据)
5. S 也计算hash(1，2步中的数据)，判断是否和C端发来的一致，并将随机数1，2，3通过两端间确认的算法生成对称加密用的密钥
6. S -> C hash(1，2，4步中的数据)
7. C 也计算hash(1，2，4步中的数据)判断是否和S端发来一致，并将随机数1，2，3通过两端间确认的算法生成对称加密用的密钥
8. 至此C和S间就都生成了接下来对称加密用的密钥了，就能进行加密传输了

为什么最后通信选择的是对称加密，因为对称加密性能比非对称加密好

### GET/POST

GET和POST方法没有实质区别，只是报文格式不同

- GET：用于获取数据，是无副作用的，具有幂等性，且可缓存
- POST：用于修改数据，有副作用，非幂等，不可缓存

GET使用URL/Cookie传参，而POST将数据放在Body里：这是HTTP协议的约定，事实上GET也可以设置Body

GET方式提交的数据有长度限制，而POST的数据则可以非常大：HTTP协议明确对HTTP头和Body不限制大小，而大小限制往往是服务器为了性能设定的

POST比GET安全，因为数据在地址栏上不可见

### URI/URL

URI：统一资源标识符，用来唯一的标识一个资源
URL：统一资源定位器，它是一种具体的URI，即URL可以用来标识一个资源，而且还指明了如何locate这个资源

### 浏览器输入域名到页面返回的全过程

1. 浏览器向DNS服务器请求解析该URL中的域名所对应的IP地址(迭代：DNS中找不到让浏览器去自己请求上一级DNS服务器；递归：DNS中找不到DNS自己去请求上一级)
2. 解析出IP地址后，根据该IP地址和默认端口80，和服务器建立TCP连接
3. 浏览器发出读取文件(URL中域名后面部分对应的文件)的HTTP请求，该请求报文作为TCP三次握手的第三个报文的数据发送给服务器
4. 服务器对浏览器请求作出响应，并把对应的html文本发送给浏览器
5. 释放TCP连接
6. 浏览器进行渲染

### Socket

端口(Port)来标记不同的网络进程

{IP:Port} => 套接字 Socket

Socket是抽象的概率，表示TCP连接的一端

TCP = {Socket1 : Socket2}
    = {{IP:Port} : {IP:Port}}

### WebSocket

HTTP存在个缺陷，即通信必须由客户端发起，即使是keep-alive，但依旧是一个request对应一个response，并且HTTP还是无状态的，每次请求服务器都要带上自己的身份标示

如果使用HTTP客户端希望服务器有消息了就通知自己，那么只能自己不断轮询服务端

而WebSocket通过一次HTTP请求建立了TCP连接后，此时服务器就可以无限给客户端发信息了，并且由于是在一个HTTP请求中，相当于是有状态的了

### HTTP 1 1.1 2 3

HTTP 1.1: 
- 默认使用connection: keep-alive，使得TCP连接建立后可以重复发生请求
- 可以在头部传一个range，请求部分内容，断点续传

HTTP 2: 
- 使用二进制传输数据，而不再是文本形式
- 多路复用：HTTP1.1中一个请求必须收到回复后才能发下一个请求，现在可以同时发多个请求而不用等待服务器返回，同一个域名只需要一个TCP连接(以前要6个)，且帧还可以乱序发送
- 服务器可以主动给客户端推送内容
- 头部压缩

HTTP 3: 由TCP转为QUIC(QUIC是谷歌对UDP的增强)

## Java基础

### 深拷贝和浅拷贝

### 类的实例化基础

1．父类静态成员和静态初始化块 ，按在代码中出现的顺序依次执行
2．子类静态成员和静态初始化块 ，按在代码中出现的顺序依次执行
3．父类实例成员和实例初始化块 ，按在代码中出现的顺序依次执行
4．父类构造方法
5．子类实例成员和实例初始化块 ，按在代码中出现的顺序依次执行
6．子类构造方法

### 内存溢出

- 堆区满了，可以在JVM参数中设的大一点
- 永久代满了，也可以设大一点

排查内存异常问题

1. 找出Java的进程ID
2. 用jstat查看该进程中的各个区的占用情况，比如栈，堆，永久代
3. 用jmap找出存活对象，异常对象往往是原因

### new String("abc") 创建了几个字符串对象

将创建1或2个字符串。如果池中已存在字符串常量“abc”，则只会在堆空间创建一个字符串常量“abc”。如果池中没有字符串常量“abc”，那么它将首先在池中创建，然后在堆空间中创建，因此将创建总共2个字符串对象。

### 重写equals和hashcode

在重写了equals方法后，如果这个对象会用到hashcode，那么就一定要重写hashcode方法

如果只重写equals方法，那么在插入HashSet时，明明两个对象equals方法计算是相同的，但是hashcode计算出来是不同的，那么就会发生相同的对象在HashSet中存了两个

### final

### BIO/NIO/AIO

BIO：同步阻塞IO，一个线程读写数据时就会被阻塞，直到读写完成为止

当同时由多个读写任务时，就不得不启用同等数量的线程(也可以使用线程池+任务队列来优化，但是底层还是同步阻塞的)

NIO：同步非阻塞IO，多路复用

Channel/Selector/Buffer

BIO是面向流的，NIO是面向缓冲区buffer的：NIO中数据先到buffer中，因此读的时候不像BIO一个字节或多个字节读，而是能一块一起读出来(NIO可以直接使用Native函数库直接分配堆外内存，然后通过DirectByteBuffer来直接对这块内存进行操作，避免了数据在Native内存和Java堆之间的来回复制)

BIO是阻塞的，NIO是非阻塞的：一个线程读写数据时如果buffer中读不到数据，就直接返回，不会阻塞

当同时多个读写任务时，通过一个线程while轮询所有读写请求，只有在线路上真正有可读写的数据时，再开线程去处理

AIO：异步非阻塞IO

相较于NIO，AIO会在数据准备完后主动通知数据使用者，这样就省去了NIO中的轮询线程操作

### JDK动态代理/cglib代理

### 注解的原理

### 序列化/反序列化原理

[参考](https://www.jianshu.com/p/3e3d86716f76)

### 强引用/软引用/弱引用/虚引用

强引用：比如`Object o = new Object()`就是个强引用，不会被GC回收

软引用：SoftReference，对于软引用关联着的对象，只有当JVM内存不足时会将它GC回收

弱引用：WeakReference，相对于软引用生命周期更短，当一旦发现对象只有弱引用，那么就会GC回收掉

虚引用：PhantomReference，虚引用不会影响对象的生命周期，如果对象只有虚引用，那么马上就会被GC回收掉

虚引用配合引用队列（ReferenceQueue）使用，当准备回收一个对象时，如果它还有虚引用，会讲虚引用加入引用队列，那么就可以通过判断是否在引用队列中来了解该对象是否要个回收了，并做相对应的工作

### StringBuilder/StringBuffer

底层维护一个char[]

StringBuilder线程不安全，StringBuffer线程安全(通过给方法都加上synchronzied关键词)

此外在String中`+`操作的字符串内容不确定时(加了个非final的变量)，编译后会使用StringBuilder和append方法来构建字符串

## Java集合

### ArrayList

ArrayList底层是一个数组，当初始化时维护的数组是空的，只有到第一次添加元素时，会被初始化为自定义值或默认值10。

当容量满了后会扩充到1.5倍容量，因此当数组容量已经很大时再扩容容易造成OOM

ArrayList频繁扩容导致性能下降 => 可以通过在初始化时就设定好容解决

ArrayList支持随机读取，读取O(1)，写入O(n)，而LinkedList写入O(1)，读取O(n)

Vector与ArrayList相比除了线程安全外，Vector每次扩容2倍，而ArrayList为1.5倍，此外还可以使用Collections.synchronizedList来转为线程安全的

### HashMap

拉链法：本质是个数组，但这个数组中的节点有个next，冲突时，冲突时就挂在next后面

成环/死循环: JDK1.7中两个线程一同resize时，链表原来是1 -> 2 -> 3，被倒序时因头插法的原因resize后变成了3 -> 2 -> 1，那么2 -> 1和1 -> 2之间就形成了环

红黑树：AVL树这颗树更平衡，查找快，插入慢，但AVL树的旋转是O(logn)，而红黑树只需要两次，因此综合选择红黑树

8转换为红黑树：因为红黑树节点相对于链表节点占用空间更大，而链表出现8的概率为千万分之一，过早转化为红黑树反而更浪费性能

hash算法：hashcode高16位不动，低16位和高16位异或

线性探测法：如果发生冲突，就向后寻找空的位置插进去

红黑树(二三树)：
- 每个节点非黑即红
- 根节点一定是黑色的
- 红色节点的子节点一定是黑色的
- 每个子节点到根节点经过的黑色节点数量一定是相同的(二三树中的树高)

hashtable：为每个方法加上synchronized来保证并发安全，但同时因为公用一把锁，所以性能很低

HashMap中的容量和负载因子：

默认是16，负载因子默认是0.75

HashMap中容量必须是2^n，因为HashMap中通过hashcode计算位置时不是%运算，而是&运算，提高计算效率。因此即使自定义了HashMap容量后其内部还是会找比该值大的第一个2^n值作为真正的容量

又因为容量必须是2^n，因此负载因子为0.75(3/4)两者乘积一定能保证是整数

HashMap中存储是无序的，如果需要有序可以使用LinkedHashMap(继承HashMap的Node，添加了before和after两个节点)/TreeMap(底层红黑树)

## Java并发

### 死锁的四个条件

1. 资源是互斥的，同时只有一个线程可以持有
2. 占有且等待，即线程持有互斥锁的同时还在等待另一个互斥锁
3. 不可抢占，即线程持有了互斥锁后其他线程是无法强制占用该互斥锁
4. 循环等待，即A线程在等待B线程持有的互斥锁，而B线程也在等待A线程持有的互斥锁

### synchronzied原理

### ReentrantLock原理

### 线程池

### wait/sleep

- 都会阻塞线程，并且都能响应中断
- wait在Object类，sleep在Thread类
- wait释放锁，sleep不释放锁
- wait需要在同步代码块中，以保证一定被唤醒，而sleep无需

## JVM

### 类加载过程

加载 -> 链接 (校验 -> 准备 -> 解析) -> 初始化

1. 加载

把class字节码文件通过类加载器加载到内存中(在加载Class时，class对象(xxx.class)是存储在堆中的，而类的元数据(method，public，变量名等)是存在方法区的)

字节码来源不限，本地/网络/动态代理实时编译的皆可

注：加载阶段和链接阶段是交叉运行的，很可能加载阶段还未执行完就进入了链接阶段

2. 校验

校验class字节码文件是否符合JVM规范，比如private/public访问权限，调用的方法是否定义了，重载是否合理，是否继承了final修饰的类

3. 准备

为静态变量(即类变量，不是实例变量)分配内存，并赋予初值(不是代码中的初始值，而是类型的初值，比如int为0，引用类型为null)

4. 解析

将符号引用替换为直接引用，即将类的类名，方法名，字段名替换为具体内存中的地址

5. 初始化

初始化开始的时机：new创建对象/获取static变量/调用static方法/反射/虚拟机启动时会初始化main方法所在类

简单说就是：对static的变量和代码块进行初始化

如果在初始化一个类时，其父类未初始化，会先初始化其父类

### 类加载器

JVM中内置了三个类加载器，除了BootstrapClassLoader其他所有类加载器都继承于java.lang.ClassLoader

1. BootstrapClassLoader(启动类加载器)：最顶层的加载类，由C++实现，负责加载`%JAVA_HOME%/lib`下的jar包或`-Xbootclasspath`参数指定的类
2. ExtensionClassLoader(扩展类加载器)：负责加载`%JRE_HOME%/lib/ext`下的jar包，比如`javax`包
3. ApplicationClassLoader(应用程序类加载器)：面向用户的类加载器，负责加载classpath下的所有jar包

双亲委派模型：

每一个类都有一个对应他的类加载器。JVM中ClassLoader协同工作时默认使用双亲委派模型。

在类加载时，JVM会判断该类是否已经被加载过了，已经被加载过的类会被直接返回，而没加载过的类会把加载请求先委派给该类加载器的父加载器的loadClass()处理，因此所有类加载请求都会被传递到BootstrapClassLoader中，当BootstrapClassLoader无法处理时，会再委派给其子类(即委托上来的类)，一层层再委派下去直到某个类加载器加载成功，就立即返回

![双亲委派模型](./pics/classloader.png)

ApplicationClassLoader的父类加载器为ExtensionClassLoader，而ExtensionClassLoader的父类加载器为null，null并不代表没有父类加载器，而是BootstrapClassLoader

双亲委派模型好处：

1. 避免类重复加载，加载过的会直接返回
2. 保证Java核心API不会被篡改，比如用户也编写一个java.lang.Object类，系统就会出现两个不同的Object类

当然可以通过自定义类继承ClassLoader来使用自己的类加载策略，而不是双亲委派模型

JDBC中破坏了双亲委派模型的原因：

因为Driver接口是JDK中定义的，而其具体实现是由厂家实现的，但根据类加载机制，当被加载类1引用了别的类2时类1和类2都会使用类1的类加载器加载，类1Driver在JDK中，而具体实现类2在其他包中，那么由于类1在BoostarpClassLoader中加载，而BoostarpClassLoader中找不到类2，因此会加载失败

注：当A类分别被两个不同的ClassLoader加载进了JVM(这两个ClassLoader肯定不能是父类调用关系)，此时如果被不同类加载器加载进来的之间没有联系。引用，那么是可以正常运行的，但是如果发生A = A，两个A是来自不同类加载器，JVM会校验它们的类加载是否相同，如果不同会报错

总结：只有是同一个类加载器加载进来的才是同一个类

### Java对象的创建过程

类加载检查 -> 分配内存 -> 初始化零值 -> 设置头对象 -> 执行init方法

1. 类加载检查

当遇到new指令时，会检查这个指令的参数是否能在常量池中定位到这个类的符号引用，并检查和这个类是否被加载/链接/初始化，如果没有就执行相应的类加载过程

2. 分配内存

对象所需的内存大小在类加载完成后便可确定，因此可以从堆中划分出制定大小内存

内存分配的两种方式：
- 指针碰撞：堆内存规整(对应GC收集器算法是标记-整理(复制)，Serial，ParNew)
- 空闲列表：堆内存不规整，存在内存碎片(对应GC收集器算法是标记-清理，CMS)

内存分配并发问题：
- CAS失败重试：乐观锁+自旋
- TLAB：为每一个线程现在Eden区分配一块内存，那么线程在分配内存时现在这块内存中分配，直到用完再用CAS方法进行分配

3. 初始化零值

JVM将分配到的内存空间都设为零值，这样就能保证该对象在Java代码块中可以不赋初始值就使用

4. 设置头对象

例如该对象是什么类的实例，对象的hash，对象的GC年龄等信息需要设置到对象头中，还有就是synchronzied用到的无锁/偏向锁/轻量级锁/重量级锁的头对象设置

5. 执行init方法

按照程序员的意愿进行初始化，执行构造函数

### JVM内存结构

![JVM内存结构](./pics/jvm-memory-structure.jpg)

线程私有：程序计数器，虚拟机栈，本地方法栈
线程共享：堆，方法区

1. 程序计数器

存放着当前线程所执行到的字节码的行号

2. Java虚拟机栈

每次方法调用的数据都是通过栈传递的

Java虚拟机栈是由一个个栈帧组成，而每个栈帧中都拥有：局部变量表、操作数栈、动态链接、方法出口信息

3. 本地方法栈

虚拟机栈为虚拟机执行Java方法（也就是字节码）服务，而本地方法栈则为虚拟机使用到的Native方法服务

4. 堆

这是JVM管理的最大的一块内存，几乎所有的对象实例和数组都会存储在堆中

![新生代和老生代](./pics/jvm-heap.png)

堆中分为新生代和老生代，而新生代中又分为了Eden和Survivor1和Survivor2

对象首先被分配在Eden区，其中大多数对象很快就会消亡，Eden区是连续的内存空间，因此分配速度很快

当Eden区满了执行Minor GC，根据GC Root将仍存活的对象都复制到Survivor0中(Survivor0和Survivor1之间一定有一个是空的)

此后每当Eden区满了后，就执行Minor GC将Survivor0和Eden中存活的一起复制到Survivor1中

接着只要Eden区满了，就执行Minor GC将Survivor1和Eden中存活的复制到Survivor0中，如此循环，每次复制都会给这个对象年龄 + 1

当对象在两个Survivor区间切换到了MaxTenuringThreshold设定值时(默认15)，就会被复制到老年代中

GC Root(可达性分析算法)：

用于判断对象是否存活，通过一系列GC Root对象(系统类加载器，栈中对象，激活状态的线程)作为起点，基于对象的引用关系，从GC Root开始向下搜索，所走过的路径称为引用链，当一个对象没有任何引用链相连，那么这个对象代表已死亡

新生代到老生代的判断条件：

在Survivor0和Survivor1之间交换次数达到15次

如果对象大小大于Eden的一半，就直接分配在老生代中

Minor GC后Servivor仍然放不下，也会放入老生代中

动态年龄判断，大于等于某个年龄的对象超过Survivor的一半时，大于这个年龄的对象全部复制到老生代

1. 方法区

用于存储已被虚拟机加载的类信息、常量、静态变量等信息

在1.7中永久代是方法区的实现，在1.8中转而使用元空间，而元空间是直接的内存(永久代是JVM管理的，因此上限受限于JVM管理的内存大小，当类信息太多时会导致内存不足，但元空间是直接的系统内存，大小受限于系统，那么出现内存不足的可能性就远远降低了)

运行时常量池：

运行时常量池是方法区的一部分，例如String的常量池

因为原先常量池在方法区中，因此受限于方法区的内存限制，在1.7及之后的版本中在堆中开辟了一个区域来存放运行时常量池

### 垃圾回收方法

1. Serial/ParNew

![Serial-ParNew](./pics/serial_parnew.png)

用于新生代的GC算法(Minor GC)，采用标记-复制算法，GC时将所有GC Root链接到的(表示存活的对象)复制到Survivor中

标记-复制算法速度慢，但是不会生成内存碎片，而新生代中对象往往很快消亡，因此适合新生代

Serial/ParNew在GC时会导致STW(Stop The World)，即应用线程被停止来执行GC

ParNew是Serial的并发版本，支持多线程

Serial/ParNew是Young GC，触发条件是Eden区满了

2. CMS

![CMS](./pics/cms.png)

用于老生代的GC算法，采用标记-清除算法，在GC时将所有未被GC Root链接到的对象全部清除

初始标记：会导致STW，标记老生代中由GC Root引用或新生代中的引用的所有对象

并发标记：和用户线程并行，遍历初始标记中存活对象，递归标记这些对象的可达对象

在并发标记过程中，由于和用户线程并行的，因此可能产生新的对象或对象引用关系发生改变，为了提高重新标记的速度，在这个阶段会将发生改变的Card设为Dirty Card

并发预清理：遍历上一步中的Dirty Card，标记其中引用的对象

可中断的并发预清理：继续遍历标记Dirty Cardy中引用的对象并将Survivor中新对象所引用的对象也标记，目的就是减轻最终标记的STW时间

最终标记：会导致STW，重新扫描标记之前并发预处理中没标记的更新对象

并发清理：标记-清理算法，与用户线程并行，但会导致内存碎片

并发重置：重置CMS状态

CMS是Old GC，触发条件为默认每两秒判断当前老生代中内存占用率是否到达阈值，到了就触发

CMS的GC时间过长问题：

CMS的GC停顿时间80%都在最终标记阶段，常见原因是新生代对老生代的无效引用(新生代的对象已经死亡)，而导致这原因是没来得及Young GC，因此可以设定在最终GC前去执行一次Young GC

当新生代在执行垃圾回收时，老生代中却没有空间来存储晋升的对象，这就会引起Full GC(Full GC会回收新生代，老生代还有永久代/元空间中的内存，Full GC采用的是Serial GC。单线程回收，对年轻代是复制-清除算法，老年代是标记-整理)

3. G1

![G1](./pics/g1.png)

G1主要为了解决大内存的STW过长问题(内存小，复制/标记的STW时间短，但内存大了，时间就长了)

G1把堆划分成了大小相等的Region，每个Region都有可能是老生代/Eden/Survivor，这样在GC时可能只处理一部分Region，回收掉回收价值高的Region，从而降低GC的STW时间

Young GC(新生代)：当Eden区满了，标记-复制算法

Mixed GC(老生代)：当老生代到达阈值，会根据全局并发标记阶段计算出来的回收收益高的Region进行GC，标记-压缩，从而解决了CMS回收后的内存碎片问题

G1中维护着RememberSet来记录Region和Region之间的引用关系，这样在标记的时候就不用扫描整个堆，但是RememberSet还依赖于Write Barrier(写屏障)来保证GC移动对象时去更新RememberSet，消耗性能

4. ZGC

引入了有色指针的概念，里面的所有指针都会有几位来保存颜色信息

与标记对象的传统算法相比，ZGC在指针上做标记，在访问指针时加入Load Barrier（读屏障），比如当对象正被GC移动，指针上的颜色就会不对，这个屏障就会先等待该指针移动好(相当于部分GC完成)，然后再返回移动好的指针位置，也就是，永远只有单个对象读取时有概率被减速(等待该指针移动好)，而不存在为了保持应用与GC一致而粗暴整体的Stop The World

### JVM调优

### JVM启动参数

## 数据库

### sql执行流程

- 连接器(建立TCP连接，查权限)
- 查询缓存(key-value形式)(不建议使用，因为一旦更新就要清空)
- 分析器(分析语法和库/列名是否存在)
- 优化器(索引选择，join的驱动表和被驱动表选择)
- 执行器(去调用InnoDB引擎接口)

### ACID

1. 原子性：一个事务要么成功要么失败
2. 一致性：事务是从一个正确的状态到另一个正确的状态
3. 隔离性：并发事务之间相互影响的程度
4. 持久性：每次事务提交后保证不会被丢失

### 索引好处

- 减少查询需要扫描的数据量
- 减少服务器的排序操作和创建临时表的操作(group by/order by/join)
- 将服务器的随机IO变为顺序IO

### 范式

1. 第一范式：属性不可再分
2. 第二范式：非主属性对码的部分依赖，即复合主键中一个字段就已经能确认某个字段
3. 第三范式：非主属性对码的传递依赖，即A确定B，B又能确定C，那么A和C存在传递依赖
4. BCNF：主属性对码的传递或部分依赖

### utf8/utf8mb4

### InnoDB和MyISAM

- InnoDB主键是聚集索引，而MyISAM中主键是非聚集索引
- InnoDB支持事务，MyISAM不支持
- InnoDB有WAL技术，通过redo log来保证crash safe
- InnoDB中count是每次统计的，MyISAM中会保存count(*)这个值
- InnoDB锁的最小粒度是行锁，而MyISAM中只有表锁
- InooDB支持外键，MyISAM不支持
- InnoDB对于auto_increment值通过redo log中推算出来的，而MyISAM则是直接保存在数据文件中

### 二叉树/B树/B+树/B*树

B+树相对于B树：
1. 由于B+树中具体数据都存储在叶子结点下，因此每个block块中包含的key

### 索引原理

### SQL优化

- 打开MRR
- join走索引，或者到应用层hash后比较

### 索引优化

- 自增索引往往是最优解，因为不但是顺序插入，还能减少数据页空洞和数据页分裂
- 前缀索引(串索引) 比如只取某一个很长的字段的一部分作为索引(作为主键的字段不能太长)
- 索引覆盖 减少回表操作
- 最左匹配 5.6中新增 索引下推
- 如果业务层能保证唯一性，可以用普通索引代替唯一索引
- join的时候被驱动表的字段要保证有索引
- 开启MRR(Multi-Range Read Optimization)，将回表操作的随机主键读取转化为顺序读取(即先排次序)

### SQL慢的原因

1. redo log满了/读数据时内存慢了，要刷进磁盘
2. 长事务，undo log太多
3. 等MDL锁，即其他事务在做DDL操作 通过show processlist查看当前状态
4. 等行锁，默认读取不加锁，通过lock in share mode/for update加锁
5. 错选索引
   1. select * from t where (a between 1 and 1000) and (b between 50000 and 100000) order by b limit 1 中有order by
   2. 索引的行数不是精确计算的，而是随机选择N个数据页，计算平均值后乘以总页数得到基数，普通索引有个回表过程，因此优化器可能计算结果后选错了索引
6. 能走索引却没走索引
   1. 索引参与计算
   2. 索引参与函数运算
   3. 隐式类型转换
   4. 隐式字符编码转换

### 聚集索引

### auto_increment好处

### 锁

- 表锁
- MDL锁：针对DDL操作，修改DDL时要加写锁，读取数据要加读锁
- 行锁：两阶段锁，锁是在事务结束后才释放的

### 隔离级别

- 读未提交：脏读
- 提交读：不可重复读
- 可重复读：幻读
- 串行化

MySQL中提交读和可重复读都是使用MVCC实现的

提交读：在每句语句开始前建立版本
可重复读：在每个事务开始前建立版本

### 脏读和幻读

脏读：读到了别的事务还未提交的内容
幻读：在更新操作时是当前读，因此实际操作结果会与前面读出来的预期不同

### 主备/读写分离

半同步复制(semi-sync)来解决备库不能及时收到主库的数据(但只能保证一主一从的模式，不能保证一主多从的模式)

1. 在事务提交时，主库把bin log发给从库
2. 从库接收到bin log后，发给主库个ack，表示收到了
3. 主库接收到这个ack后才能给客户端返回事务完成的确认

### 分页

limit 0, 5 => (1 ~ 5)
limit 5 => (1 ~ 5)
limit 10, 15 => (11 ~ 25)
limit 15 offset 10 => (11 ~ 25)

分页优化：

1. 利用覆盖索引，减少回表
2. 如果主键是递增的(auto_increment)，那么可以先查出主键，然后用in，join，between，大于小于符号 方式查出具体数据
3. 对于offset很后面的内容，使用倒序(DESC)来查询

### explain

1. select_type

查询类型

- Simple：不带子查询或union查询
- Primary：该查询是最外层的查询
- Union：表示是union第二个或后面的查询
- Subquery：子查询中第一个select

2. table

查询涉及的表或衍生表

3. partitions

匹配的分区

4. type

- system：表只有一行，const的特殊情况
- const：最多只有一行匹配，比如使用主键或唯一索引
- eq_ref：只匹配到一行
- ref：符合索引的最左前缀
- range：范围查找
- index：全表扫描，但扫描的是索引树
- all：全表扫描

type性能如下：

ALL < index < range ~ index_merge < ref < eq_ref < const < system

5. possible_keys

在该查询中可以用到的索引，但正真用到的索引在key中显示

6. key

正真被用到的索引

7. key_len

使用到扽索引的字节数，可以用来判断复合索引中最左前缀匹配到那些字段

8. ref

表示表查找用到的常量const，字段，函数

9. rows

估算的需要扫描到的行数

10. filtered

表示该查询条件过滤的数据的百分比

11. extra

- using filesort：order by/group by没能走索引，使用内存/文件排序
- using temporay：用了临时表保存中间结果
- using index：用到了覆盖索引
- using index condition：用到了索引下推
- using where：表示存储引擎返回给MySQL服务层后再做where过滤
- using join buffer：使用了join的连接缓存，BNL/BKA

### 全文索引

InnoDB和MyISAM都支持全文索引 FULLTEXT

当使用like %的形式搜索很慢时，就需要全文索引

全文索引主要思想就是首先用户先设定需要全文索引的字段，接着通过分词的方法建立起来一个关键词(分词的结果)和主键的关联表，接着用户通过全文索引查询时就去这个表里搜索，找到关键词就能直接对应到主键

全文索引有要求全文索引的字段最小长度，InnoDB中默认是4，只有在这个长度之上才会被加入关键词-主键表中

### MVCC

### 错选索引

优化器会通过扫描行数、临时表、是否排序等因素进行综合判断选择索引

1. select * from t where (a between 1 and 1000) and (b between 50000 and 100000) order by b limit 1 中有order by
2. 索引的行数不是精确计算的，而是随机选择N个数据页，计算平均值后乘以总页数得到基数，普通索引有个回表过程，因此优化器可能计算结果后选错了索引

解决方法：

1. 使用analyze table T让MySQL重新统计索引
2. 使用force index(I)来强行使用指定索引
3. 新增或重建索引

### 日志表索引

1. 都是insert，使用int型主键，顺序写入
2. 设置每个页不再留1/16给后面用(空洞)
3. 索引不宜过多，不然insert时还要维护索引
4. 按时间分库
5. 查询和增删改区分表

## 设计模式

### 设计模式原则：

- 开闭原则：开放扩展，关闭修改

### 工厂模式

创建对象的过程对调用者透明，外界给出信息就能返回正确的对象

简单工厂模式：通过case匹配类名字符串/.class

工厂方法模式：所有工厂类继承同一个抽象工厂，实现生产东西的方法，通过new不同的工厂，调用相同的方法，取得不同的对象

抽象工厂模式：相较于工厂防范模式，其工厂的实现类中提供了一整套产品，这些产品是相互依赖或有关系的

## 分布式

### RPC、消息队列、缓存

 

## 向面试官提问

- 我个人的薄弱环节，怎么提升，未来职业规划
- 业务和技术
- 给我的简历一点建议

## 算法

- 反转域名
- 疯狗问题
- 判断素数，不能用除法：用个数组索引代表数字，填表

### 布隆过滤器

布隆过滤器能告诉你某样东西一定不存在或者可能存在

会对一个值进行hash多次(不同的hash法)，从而在bitmap中对对应的置1，而在查询数据时也会多次hash，找到对应位置上是否为1，只要多次hash出来的值有一个不为1，那么它就一定不存在，但是如果都为1，是不能保证一定存在(因为可能是别的值hash后将这个位置置1的)

### LRU

是一种缓存淘汰策略，简单说就是最近用过的就是有用的，一直没用的就是可以被删除的

通过HashMap + 双向链表实现

### 优化尾递归

1. 优化取中间点数字的算法
2. 当递归到left和right间数字较少时改用插入排序
3. 尾递归，就是让递归方法后不再依赖环境

```java
while(low < high) {
    pivot=Partition(list,low,high);
    quickSort(list, low,pivot - 1);
    //quickSort(list,low,pivot-1); 原递归调用
    //quickSort(list,pivot+1,high);
    low = pivot+1; // 尾递归
}
```

## 场景题

### 排查CPU100%

1. top查看CPU占用高的进程pid
2. top -Hp pid来找到这个进程中是哪个线程占用率高，找到这个线程的pid
3. jstack -l pid查看对应线程的堆栈信息
4. 堆栈信息中nid对应的就是pid的16进制

### TopK

- 快排
- 快排优化，下一次递归时只要递归前或后半部分
- 堆排序

当数据量很大时

- 堆排序，维护大小为K的堆，每次取一个和堆顶的值比较是否要插入堆中



