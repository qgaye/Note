# Redis笔记

## 数据类型和底层数据结构

Redis使用全局哈希表来保存所有的键值对，全局哈希表中value保存的是(key, value)的键值对的指针

![全局哈希表](./pics/global_hash_table.jpg)

当key出现哈希冲突时，在全局哈希表中会通过链地址法(即哈希冲突的entry会以链表的形式保存)，那么此时当查询到哈希冲突的key时就需遍历整个链表才能找到所查找的entry，链表的遍历是O(n)，所以就非常耗时

此时Redis就会对全局哈希表做rehash动作，为了使rehash操作更高效，Redis默认使用两个全局哈希表并采用渐进式rehash。全局哈希表1和全局哈希表2，当一开始插入数据时都插入全局哈希表1，当需要rehash时：
1. 首先给哈希表2分配更大的空间(即增加桶的数量减少哈希冲突的情况)
2. 然后并不是直接把哈希表1中的数据全部拷贝到哈希表2(因为做全量拷贝时就会造成Redis线程阻塞而无法服务其他请求)，而是Redis仍正常处理客户端请求，每处理一个请求时将哈希表1中第一个索引(即桶)上的所有entry拷贝到哈希表2，然后等到处理下一个请求再将哈希表1中第二个索引上的所有entry全部拷贝到哈希表2，以此类推
3. 最后等到哈希表1中全部拷贝到了哈希表2后，释放哈希表1中空间

通过渐进式rehash，将一次性的大量拷贝的开销，分摊到多次处理过程中，保证了Redis的快速响应

全局哈希表中的每一项都是个dicEntry结构体，用来指向一个键值对，dicEntry有三个8字节的指针，分别指向key、value和下一个dicEntry(哈希冲突的链地址实现使用)

![RedisEntry](./pics/redis_entry.jpg)

虽然dicEntry中有三个指针分别8字节，应该只占用24字节，但实质上因为Redis使用的内存分配库jemalloc，其会根据申请的字节数N，找一个比N大但最接近N的2次幂作为分配空间以减少频繁分配的次数，因此申请24字节，但jemalloc会分配32字节，也就是dicEntry结构体会最终占用32字节

- String：简单动态字符串
- List：双向列表 / 压缩列表
- Hash：哈希表 / 压缩列表
- Sorted Set：跳表 / 压缩列表
- Set：整数数组 / 压缩列表

跳表：其在链表的基础上，增加多级索引，通过索引位置的跳转实现数据的快速定位，整个查询过程就是在多级索引上跳来跳去，因此叫做跳表，跳表查找的时间复杂度为O(logN)

![跳表](./pics/skip_list.jpg)

对于所有Redis的操作来说，对单个元素的操作基本都是O(1)，比如HGET，HSET，SADD，SREM等；对于范围操作，比如HGETALL，SMEMBERS等返回集合中所有数据的操作，是需要遍历整个集合的，复杂度一般都为O(N)，因此尽量避免；对于统计操作，比如LLEN，SCARD统计集合中元素个数的复杂度都是O(1)，Redis可以高效完成；此外比如对集合首尾元素的操作，因为压缩列表的支持，复杂度也只有O(1)

-----

Redis的数据类型有很多，并且不同的数据类型都有相同的元数据需要记录(比如最后一次访问的时间、被引用次数等)，所以Redis中会用一个RedisObject结构体来统一记录这些元数据

RedisObject包括8字节的元数据8字节的指针，指针进一步指向具体的数据类型的实际数据，比如string就是指向SDS结构所在地址

![RedisObject结构体](./pics/redis_object.jpg)

RedisObject元数据包括：
- type：表示值的类型，包括五大基本类型String、List、Set、Sorted Set、Hash
- encoding：表示值得编码方式，用来表示Redis中实现各个基本类型的底层数据结构，包括SDS、压缩列表、哈希表、跳表等
- lru：记录这个对象最后一次被访问的时间，用于淘汰过期的键值对
- refcount：记录对象的引用计数

![RedisObject元数据](./pics/redis_object_detail.jpg)

为节省空间，Redis对long类型(Redis基本类型并没有long类型，但当保存的是64位有符号整数时会把string类型保存为一个8字节的long类型整数)和SDS做了专门不同的设计

int编码：当保存的是long类型的整数时，RedisObject中指针就直接赋值为整数数据，这样就无须通过指针再指向整数了，节省了指针空间开销
embstr编码：当保存的是string类型数据并且字符串小于等于44字节时，RedisObject中元数据、指针和SDS会被分配到连续的内存上，避免内存碎片的问题
raw编码：当字符串大于44字节时，就会给SDS单独分配内存空间，并通过RedisObject的指针指向SDS

![RedisObject不同编码模式](./pics/redis_object_encode.jpg)

### string

当保存的是64位有符号整数时，string类型会被保存为一个8字节的long类型整数，称作为int编码方式

当保存的数据中包含字符时，string类型就会使用简单字符串SDS(Simple Dynamic String)结构来保存

![SDS结构](./pics/redis_sds.jpg)

buf：字节数组，保存实际数据，为了表示字节数组结束，Redis会自动在字节数组最后加个`\0`，这会额外增加1字节开销
len：占4字节，表示buf已用长度
alloc：占4字节，表示buf实际分配的长度

### 压缩列表(ziplist)

压缩列表表头有三个字段zlbytes、zltail和zllen分别表示列表长度、列表尾的偏移量和列表中entry个数，表尾还有个zlend表示列表结束

![压缩列表](./pics/ziplist_detail.jpg)

压缩列表是一种非常节省内存的数据结构，因为它使用一系列连续的entry保存数据，entry都是挨个放置的，因此不需要额外指针进行连接，这样就可以节约内存

此外压缩列表的优势还在于如果要查找第一个元素和最后一个元素都是O(1)的，并且查询元素个数也是O(1)，但对于其他元素的查找就还是O(N)

entry结构：
- prev_len：表示前一个entry长度，当前一个entry长度小于254字节该prev_len字段就占1字节，反之就占5字节(虽然一字节可以表示0-255数字，但是压缩列表的zlend默认值就是255，用于表示列表结束，因此255不可用，而5字节时第一个字节一定是254，所以只有前面entry长度必须小于254才可占1字节，如果可以等于254的话就会导致无法分别是1字节还是5字节)
- len：表示entry自身长度，4字节
- encoding：表示编码方式，1字节
- content：保存实际数据

Redis中基础类型比如Hash，分别有两种底层实现，分别为压缩列表和哈希表，Hash类型设置了使用压缩列表进行保存的两个阈值(hash-max-ziplist-entries表示用压缩列表保存时哈希集合中最大元素个数，hash-max-ziplist-value表示用压缩列表保存时哈希集合中当个元素的最大长度)，当超过阈值Redis就会把Hash类型的实现结构从压缩列表转为哈希表，并且一旦转为哈希表后就不会再转回压缩列表(像其他基本类型，List、Set、Sorted Set底层也是有两种实现方式，压缩列表和双向链表/整数数组/跳表，也分别有阈值用于控制何时要从压缩列表转为对应的数据结构)

压缩列表的优势就是非常节省内存，但每次查找和插入元素时都需要遍历整个压缩列表，虽然可以利用CPU高速缓存，但也不宜存储过多数据(这也就是为何在超过阈值后会转为对应的数据结构)，即时间换空间。并且因为压缩列表每个entry都是连续排列的，因此一但某个entry进行修改就会引发多个entry的联级调整，从而引发性能问题

可以利用Hash这种基础类型使用压缩列表的特性，将简单的string类型的kv键值对转为Hash类型存储(比如可以将整个key的前6位作为kv对中的key，而将后三位作为Hash类型中的key)，因为当Hash类型底层实现为压缩列表时非常节省内存，因此每增加一个value占用的内存空间比string类型小的多，但要注意控制Hash类型中的元素个数(也就是控制key位数的拆分)，防止超过阈值转化为哈希表的实现，这样就起不到节省内存的作用了
但此时就无法针对每个键值对设置过期时间(因为key被拆分成了两部分)，简单说这样就是把Redis作为了数据库而不是缓存使用，需要自行保证数据可靠、淘汰策略等方面

## 单线程

Redis的单线程主要是指Redis的网络IO和键值对读写是由一个线程完成的，这也是Redis对外提供键值存储服务的主要流程。但Redis的其他功能，比如持久化，异步删除，集群数据同步等，其实是由额外的线程执行的

单线程模式避免了多线程下共享资源的并发访问控制的问题

此外Redis使用到了IO多路复用的能力，由内核负责监听套接字上的连接请求和数据请求，一旦请求到了，就会通过回调的方法交给Redis线程处理，而不再会被阻塞在套接字上(比如阻塞在`recv()`上)

![IO多路复用](./pics/multiplexing.jpg)

但单线程处理模式也存在个两个方面的问题：
1. Redis单线程中任意一个请求耗时(比如操作bigkey)会影响到后面的所有请求，进而影响整个Redis的性能。需要业务人员自行去规避
2. 并发量大时，单线程读写客户端数据存在性能瓶颈，虽然IO多路复用是非阻塞的，但单线程读写客户端数据是阻塞的。在Redis6.0中可以在高并发场景下通过多线程读写客户端数据，但真正键值对的读写还是由单线程完成

## AOF和RDB

Redis持久化主要有两大机制，AOF(Append Only File)日志和RDB快照

AOF是写后日志，意思是先将数据写入内存，然后再记录到日志，相较于MySQL中的WAL(Write Ahead Log)写前日志，在数据实质写入前先写入redo log以便故障时进行恢复

写后日志AOF中记录的是Redis收到的每一条命令，这些命令以文本的形式保存，AOF日志中`*n`表示当前命令有三个部分，每个部分都是以`$n`开头，表示该部分所占的字节数，而每个部分中具体的内容就是具体的命令、键、值，而写前日志redo log中保存的是对数据页的修改

写后日志AOF有两个好处：
- 因为写日志前已经执行过了命令，所以如果命令执行失败那么在日志中不会记录这个错误命令，而写前日志就会存在记录了错误命令的情况
- 其在命令执行后才记录日志，所以不会阻塞当前的写操作

AOF在写回磁盘时机存在两个问题：
- 当一个命令刚写入内存但还没来得及将日志写入磁盘时就宕机了，那么这个命令和相应的数据就有丢失的风险，因此这个命令无法通过日志进行恢复
- AOF虽然避免了对当前命令的阻塞，但如果写入磁盘的操作很慢，那么就会导致后续的写命令被阻塞无法执行

AOF三种写回策略：
- Always(同步回写)：每个命令执行完成后，立马将日志同步写回磁盘
- Everysec(每秒回写)：每个命令执行完后，只是先将日志写到AOF文件的内存缓冲区，然后每隔一秒把缓冲区中内容写回磁盘
- No(操作系统控制回写)：每个命令执行完后，只是先将日志写到AOF文件的内存缓冲区，然后由操作系统决定何时将缓冲区内容写回磁盘

![AOF三种写回策略优劣](./pics/aof_strategy.jpg)

随着AOF中记录的日志越来越多就会导致AOF文件越来越大，文件过大会造成三个问题：
- 文件系统对文件大小有限制，无法保存过大的文件
- 当文件过大时，再追加命令记录效率降低
- 当使用AOF进行故障恢复时，需要重新执行AOF中所有命令日志，如果日志文件过大就会导致恢复过程非常慢

AOF重写机制来解决这个问题，重写的核心在于整个AOF日志中针对同一个key会存在多个操作，而这些操作只需要将他们重写成一条命令即可，比如`set a 1`和`set a 2`最后重写成`set a 2`一条即可

AOF重写过程：一个拷贝，两处日志

每次执行重写时，主进程fork出后台线程bgrewriteaof子进程，fork会把主线程内存拷贝一份给bgrewriteaof子进程，其中就包括了数据库最新的数据，子进程就可以在不影响主进程的情况下逐一将内存中数据记入重写日志。因为主进程并未被阻塞，仍然可以处理新来的操作，如果有写操作主线程会写两份日志，包括本来的AOF日志和AOF重写日志，写本来的AOF日志作用是在重写过程中如果发生宕机，此时AOF日志还是齐全的，写AOF重写日志作用是当子线程重写完成后，将AOF重写日志应用到重写完的日志中即可保证此时该日志处在数据库最新的状态

![AOF重写机制](./pics/aof_rewrite.jpg)

父子进程重写潜在的阻塞风险：首先fork子进程这个瞬间是会阻塞主进程的，fork子进程需要拷贝进程必要的数据结构，比如拷贝页表(虚拟内存和物理内存的映射关系)等，但并不真正拷贝父进程拥有的内存中所有数据，fork采用操作系统提供的Copy On Write机制，避免了一次性拷贝大量数据给子进程造成长时间阻塞，也就是说虽然fork了子进程，但并没有申请和父进程相同的内存大小，此时父子进程指向的内存地址是相同的，只有当写操作发生时，才会拷贝出一份内存中真正数据提供给进程进行写操作。因此当AOF重写过程中，父进程还是有写操作命令进入，此时父进程就需要真正拷贝这个key所对应的内存数据，申请新的内存空间，内存分配是以页为单位进行的，默认是4k，因此当父进程操作的是个bigkey时就需要申请大块内存，产生阻塞，此外如果操作系统开启内存大页机制(Huge Page，默认页大小4k，大页内存页大小2M)，父进程即使操作的是个value和小的key，但因为内存分配是以页为单位，就必须至少分配2M，因此非常容易造成阻塞(Huge Page在Redis实际使用时建议关闭)

为什么重写时不复用原先的日志：因为父子进程同写一份日志必然会产生竞争问题，控制竞争就会影响父进程的性能，此外如果在重写过程中如果发送错误，复用原先的日志就会导致日志被污染，而重写一份新日志的情况下直接删除该文件即可

-----

RDB(Redis DataBase)文件记录的是某一时刻Redis内存中全量数据的快照，RDB快照相较于AOF日志，当宕机后恢复的很快，因为记录的就是内存中的数据，而AOF记录的是命令，必须将全部命令执行完后才能完成恢复

Redis提供两个命令来生成RDB文件：
- save：在主进程中执行，会导致阻塞，其他请求命令无法被执行
- bgsave：主进程fork一个子进程，专门负责写入RDB文件，避免了主进程的阻塞，是Redis RDB文件生成的默认配置

bgsave中，虽然fork出的子进程负责写入RDB文件，不会阻塞主进程，但主线程写操作并不能正常被处理，因为如果写操作更新的数据在更新前就被子进程写入RDB文件，那么这个写操作将会被丢失。Redis通过操作系统提供的Copy On Write写时复制技术(fork子进程时只会阻塞主进程申请内存空间并拷贝必要的进程数据结构，比如页表等，但主进程内存数据并不会被拷贝一份给子进程，而是读数据时共用，只有当写数据时才申请内存空间并对应的内存数据拷贝到新的内存地址上进行修改)，如果主进程只有读操作，那么bgsave子进程互不影响，当主进程有写操作时，这块数据将被拷贝一份到新的内存地址，主进程修改新的数据副本，然后bgsave子进程会把这个副本写入RDB文件，从而使得主进程可以正常执行并且数据保证不会丢失

RDB文件不易频繁生成快照，比如秒级生成，虽然bgsave不阻塞主线程，但：
- RDB文件每次都是内存数据全量写入磁盘，造成磁盘压力很大，当多个快照同时竞争磁盘带宽，更易导致性能下降
- fork子进程这个动作瞬间是需要阻塞主进程的，频繁bgsave也就是频繁fork，最终也就导致频繁阻塞主进程

因此更好的方案是快照+基于此快照的修改记录，也就是RDB+AOF，在Redis4.0中混合使用AOF和RDB快照，即快照不必频繁生成，通过AOF记录两个快照间的操作命令，并且当第二次快照生成时就可以清空AOF日志

RDB快照方案不适用于写操作非常频繁的场景，因为fork出bgsave子进程后，当主进程有写操作时需要先拷贝数据副本后进行修改，如果写操作非常频繁，那么在bgsave后台生成RDB文件过程中，就会频繁申请内存空间，导致内存很快被占用完，如果此时系统开启swap，那么Redis内存中数据就会被交换到磁盘上，性能骤降，如果没有开启swap，那么直接触发OOM，父子进程可能被操作系统直接kill掉

## 主从同步

Redis主从模式下，主从库间采用的是读写分离的方式，读操作主库和从库都可以接受，而写操作首先到主库被执行，然后主库将写操作同步给从库

### 主从第一次同步

启动多个Redis实例后，通过replicaof(Redis5.0之前为slaveof)命令形成主从关系`replicaof [master-ip] [master-port]`

![主从第一次同步](./pics/first_sync.jpg)

1. 从->主：`psync ? -1`，psync命令表示从库要进行数据同步，该命令包括主库的runID(Redis实例启动后都会自动生成一个随机ID用于唯一标识这个实例)和复制进度offset两个参数，主库根据这两个参数启动复制，第一次复制复制时runID未知所以传`?`，offset传`-1`
2. 主->从：`FULLRESYNC [runID] [offset]`，主库收到psync命令后，用FULLRESYNC命令(FULLRESYNC表示第一次复制采用全量复制)带上主库的runID和主库目前的复制进度offset返回给从库，从库记录runID和offset
3. 主->从：主库执行bgsave，生成RDB文件，将RDB文件发给从库，从库接收到RDB文件后清空当前数据库后加载RDB文件。在主库将RDB文件从库过程中主库并不会被阻塞，此时主库收到的写命令并没有记录到RDB文件中，因此主库在内存中用专门的replication buffer记录RDB文件生成后的收到的所有写操作
4. 主->从：当主库完成RDB文件传输后，将此时replication buffer中记录的写操作发给从库，从库执行这写操作后就实现了主从同步

因为每个从库第一次主从同步都需要主库完成一次RDB文件的生成和传输，如果从库数量很多就会给主库带来压力，比如fork子进程时的阻塞，传输RDB文件占用的带宽等，此时可以通过主-从-从模式将主库生成和传输RDB文件压力分摊到从库上，即从库不在直接和主库进行交互，而是和另一个从库相互完成同步`replicaof [slave-ip] [slave-port]`

### 增量同步

当网络出现抖动时，从库重连主库如果每次也都进行全量同步开销就会很大，因此通过repl_backlog_buffer环形缓冲区实现增量同步

当主库的Redis实例只有有从库连接，主库就会将所有写命令缓存到repl_backlog_buffer中，repl_backlog_buffer是一个环形缓冲区，主库会记录自己写到的位置(master_repl_offset)，从库则会记录自己已经读到的位置(slave_repl_offset)

当刚开始时主库和从库读写位置是相同的，随着主库不断接受写命令，master_repl_offset也就不断增加(offset单调递增)，从库在接收到主库发送的写命令并执行完成后也会增加slave_repl_offset，正常情况下两个偏移量应该是级别相等的。当网络抖动后从库重新连上主库后，从库会先发送psync并带上slave_repl_offset，主库将从库传来的slave_repl_offset和自己的master_repl_offset对比，将两者之间的写命令同步给从库即可。但是因为repl_backlog_buffer是个环形缓冲区，即内存大小是固定的，当不够时会覆盖掉旧的数据，如果从库断开很长时间或从库读取速度比较慢，就会导致从库还未读取的写命令已经被主库新的写命令给覆盖了(master_repl_offset偏移量大于slave_repl_offset一个环的大小)，此时从库就不得不进行全量同步以保证数据一致性

因此可见，repl_backlog_buffer大小设置非常关键，因为一旦超出限制就会导致全量同步，这是非常消耗性能的，repl_backlog_buffer大小通过repl_backlog_size参数调整，计算公式是`repl_backlog_size = 2 * (主库写入命令速度 * 操作大小 - 主从库间网络传输命令速度 * 操作大小)`，但是还是会存在覆盖未同步写命令的情况，除了进一步加大repl_backlog_size大小外，还可以使用切片集群

repl_backlog_buffer：其目的是在从库断开连接后，帮助找到主从间未同步的数据而设计的环形缓冲区，从而避免全量同步带来的性能消耗

replication buffer：无论是客户端还是从库，Redis都会为他们分别分配一个内存buffer进行数据交互，Redis先把数据写到buffer中，然后再把buffer中数据发送到client socket中完成网络发送，从而完成数据交互，而replication buffer正是Redis分配给从库用户传输主库写操作命令的buffer(Redis分配的每个buffer大小是有限制的，通过client-output-buffer-limit参数限制，当buffer超过限制主库会强制断开这个客户端的连接)

总结：repl_backlog_buffer只有当有从库的时候主库就会创建，主库的所有写命令都会缓存到repl_backlog_buffer中，当主从间不一致时，通过repl_backlog_buffer中找到需要同步的增量数据然后通过replication buffer完成发送，而replication buffer是Redis为每个client(客户端或者从库)分别创建的buffer

## 哨兵机制

哨兵机制主要就是负责当主库挂了后从从库中选举出新的主库，哨兵主要负责的就是三个任务：监控，选择主库，通知客户端

- 监控：判断主从库是否下线
- 选主：从从库中选举出新的主库
- 通知：让从库执行replicaof与新主库同步，并通知客户端和新主库连接

哨兵本质就是一个运行在特殊模式下的Redis进程，但不负责处理Redis的读写命令，其运行在独立的机器上，并且可以部署在多个不同的机器上组成哨兵集群，当主从库实例运行时它也在运行

### 监控

哨兵进程会使用ping命令检测自己和主从库的网络连接情况，当哨兵发现主库或从库对ping命令超时了(超时时间由down-after-milliseconds控制)，那么哨兵就会先把它标记为`主观下线`(从库下线影响不大，主库和其他从库还能继续服务读操作，但主库下线就会导致写操作无法被处理，因此主库下线就要求进行新主库切换)

注：超时时间down-after-milliseconds配置非常关键，如果配置过短，就会造成正常的网络抖动引发主库切换，而配置过长则会造成主库发送故障后需要很长时间后才能被哨兵感知到

选举新主库并进行主库切换是非常消耗性能的，因此如果仅仅以一个哨兵判断的`主观下线`就进行切换是不合理的，因为很可能是网络抖动等原因造成的，此时主库往往还是在正常服务的，因此可以通过部署多个哨兵形成哨兵集群来解决这个问题，并且哨兵集群还可以保证当其中的某个哨兵实例宕机后其他哨兵还可以继续服务

哨兵集群通过是否有一定数量的哨兵实例都认为该主库已经`主观下线`来判断是否该主库`客观下线`，简单来说，`主观下线`就是某个哨兵实例自己发现主库下线，而`客观下线`则是整个集群根据每个哨兵判断出来该主库是否已经下线，`客观下线`即认为是真正下线了

`客观下线`判断过程：当任何一个哨兵实例判断主库`主观下线`后，就会给其他哨兵实例发送is-master-down-by-addr命令，其他哨兵根据自己和主库的连接状态，如果也判断主库`主观下线`即做出Y相当于赞成票，反之为N相当于反对票，当收到的赞成票数大于等于哨兵配置文件中的quorum配置的值(quorum一般配置为`哨兵实例个数 / 2 + 1`)，那么就可以将该主库标记为`客观下线`

在配置哨兵时通过`sentinel monitor [master-name] [ip] [port] [quorum]`，设置主库的IP，端口，quorum是个数值

哨兵实例通过Redis的pub/sub机制来发现集群中的其他哨兵，当哨兵实例和主库建立连接后，就可以订阅频道或在频道内发布信息(频道用于区分消息类别，只有订阅了同一频道的应用才能通过发布进行信息交换)，在主库上有一个名为`__sentinel__:hello`的频道，不同的哨兵就是通过该频道实现互相发现和通信。当哨兵实例连接和主库建立连接后，就会把自己的ip和端口发步到该频道中，并订阅该频道，当其他哨兵连接到主库后就能够互相发现了，接着通过订阅到的ip和端口，哨兵间就可以相互建立连接

哨兵实例通过向主库发送INFO命令，主库接收到后将从库列表返回给哨兵(ip+端口)，接着哨兵根据从库列表中的连接信息和每个从库建立连接，并在这个连接上持续对从库进行监控

### 选主

选举出负责进行新主库切换的哨兵实例：

当某个哨兵实例发现主库`主观下线`并在哨兵集群内完成仲裁(投票)将主库标记为`客观下线`后，该哨兵可以给其他哨兵发送命令，表明希望由自己来执行主从切换，并让其他哨兵进行投票，整个过程称作Leader选举

在投票过程中任何想要成为Leader的哨兵都需要满足两个条件：第一，拿到半数以上的赞成票(包括已经宕机的哨兵节点)；第二，拿到的票数还要大于哨兵配置文件中的quorum值

整个选举过程类似于raft协议，即申请成为Leader的哨兵会首先给自己投一票，接着广播给其他哨兵，其他哨兵会给他最先收到的请求的哨兵投票，之后收到的请求不给投票，最后申请成为Leader的哨兵判断票数是否达标，如果没有，等待随机秒后进行下一轮投票

注：哨兵集群中哨兵实例不是越多越好，在判定`客观现象`和选举哨兵Leader时都需要和其他哨兵进行通信，如果哨兵实例过多就会导致网络带宽压力，也就意味着各个过程所需时间变长，切换主库的时间变长

挑选出哪个从库作为新的主库：

1. 该从库必须处在在线运行状态
2. 检查从库之前的网络连接状态，如果该从库之前经常会和主库断连，并且断连次数超过一定的阈值，就说明该从库网络状况不佳，将它选举为主库后很可能会出现网络故障导致又得重新选主
3. 通过从库优先级，用户通过slave-priority为从库分别设置优先级，比如给机器性能较好的从库设置的优先级更高
4. 通过和旧的主库的同步程度，哨兵获取每个从库的slave_repl_offset(该偏移量保存在从库中，因此即使主库宕机，该偏移量依旧可以被获取)，slave_repl_offset值越大就说明和主库同步程度越高，即更优先选为主库
5. 通过实例的runID，在优先级和同步程度相同的情况下，runID越小的更优先选为主库

### 通知

哨兵本质上就是个运行在特殊模式下的Redis实例，因此哨兵实例也提供了pub/sub机制，客户端可以通过哨兵订阅消息，哨兵提供的消息频道很多

![哨兵提供的重要频道](./pics/sentinel_channel.jpg)

客户端通过订阅`+switch-master`频道后，当哨兵集群选举出了新的主库后，客户端就能够收到switch-master事件消息，其中就会包含新主库的IP和端口，此时客户端就能够及时感知到主库进行了切换并和新主库建立连接进行通信

### 切换主库影响

客户端读写分离的模式下，读操作不受影响，依旧可以打到从库上，因为主库已经挂了而且还没有选出新的主库，此时写操作就会失败，失败的持续时间 = 哨兵切换主从的时间 + 客户端感知到新主库的时间

如果不想让业务端感受到异常，客户端可以先将写操作缓存起来或者写入消息队列，等待新主库切换完成后再将写操作发送给新的主库，但这种对于业务的局限性比较强，要求对写入的数据要不敏感，并且如果积压的写操作很多就会导致切换后执行这些积压操作消耗大量时间

## 切片集群

Redis应对数据量增大的情况有两种方案：
- 纵向扩展：升级单个Redis实例机器的内存和CPU配置
- 横向扩展：增加Redis实例个数，数据会被写到不同的Redis实例上

纵向扩展实施简单，但是收到硬件很成本的限制，并且当单机数据量庞大时，fork子进程过程就会造成长时间阻塞(fork需要申请拷贝页表等进程必要的数据结构，数据量大也就是页表大)，因此在面对百万级别的用户规模时，横向扩展Redis切片集群是更好的方案

在Redis3.0开始，官方提供了Redis Cluster方案来实现切片集群，即解决数据和切片实例的对应关系，即数据应该到哪个实例上去读写

Redis Cluster方案采用哈希槽(Hash Slot)来处理数据和实例间关系，一个Redis切片集群固定有16384个哈希槽，首先将key按照CRC16算法计算得到一个16bit值，然后将这个16bit值对16384取模，得到0～16383间的模数，模数即对应了相应编号的哈希槽(`CRC16(key) % 16384`)

注：在手动分配哈希槽时需要将16384个槽都分配完，否则Redis集群无法正常工作

Redis实例会将自己的哈希槽信息发给和它相连的其他实例，来完成哈希槽分配信息的扩散，当客户端连接到一个Redis实例后，实例就会把哈希槽的分配信息发给客户端，从而无论是Redis实例还是客户端都能够拥有哈希槽的分配信息，并且客户端可以在本地就根据key计算得到相应的哈希槽，然后将请求发给对应的Redis实例

Redis Cluster提供重定向机制来解决当哈希槽重新分配时(当切片集群扩容)让客户端能够感知到的问题，当客户端将某个key的请求发给一个Redis实例后，如果该Redis实例上没有该key对应的哈希槽，那么就会返回一个MOVED命令`MOVED [hash-slot] [ip:port]`，然后客户端将请求重新发给返回的ip的Redis实例，并更新客户端本地哈希槽和Redis实例的映射关系。如果客户端请求时，哈希槽迁移还未完成，即比如某个哈希槽需要被分配给一个新的机器(新Redis实例)，那么就需要将该哈希槽上在旧机器上的数据迁移到新机器上，在这个迁移过程中，Redis实例不会返回MOVED命令而是返回ASK命令`ASK [hash-slot] [ip:port]`，然后客户端需要先向新的Redis实例发送ASKING命令，然后才能重新将key操作命令发给该机器，并且ASK命令不会更新客户端本地缓存的哈希槽分配信息

切片集群的主从：每个切片Redis实例都作为一个主库，然后为每个主库分别再建立多个从库，即整个集群存在多个主库，每个主库自主拥有一套主从集群

总结：Redis集群方案两大核心问题：请求路由和数据迁移

请求路由：像Redis Cluster在客户端和Redis实例都缓存路由信息(即哈希槽分配信息)，便于客户端直接找到key对应的实例，但要求客户端SDK必须支持；像Codis是中心化模式，即增加了Proxy层，所有Redis实例都挂在Proxy层后，客户端请求都打到Proxy层上，由Proxy层完成路由分发，这样客户端对路由就无感了，可以像操作单个Redis一样，但增加一次Proxy会带来性能损耗

数据迁移：Redis Cluster还是Codis都需要服务端和客户端/Proxy层相互配合，迁移过程中，Redis实例针对正在迁移的key会让客户端/Proxy层重定向访问，从而能够保证迁移过程中key访问不受影响，但会增加访问延迟，迁移完成后，Redis Cluster会让客户端也感知到并更新本地缓存，而Codis则直接在Proxy层更新路由表即可，客户端在整个过程中无感。此外在迁移过程中还需要关注迁移超时、迁移失败这些异常情况和加快迁移速度、bigkey处理这些性能问题

## 数据类型的选择

### 聚合统计

Set和Sorted Set，但Sorted Set只支持交集、并集，不支持差集运算，而Set都支持

### 排序统计

List和Sorted Set，List基于插入先后顺序进行排序的，而Sorted Set基于元素的权重进行排序，当分页查询时，因为List是根据偏移量进行定位的，因此如果新插入了元素就会导致原先所有的元素的偏移量发生变更，而Sorted Set是根据权重，因此不会出现这个问题

### 二值状态统计

Bitmap，Bitmap本身是用String类型作为底层数据结构实现的一种二值状态的数据类型，因为String底层会被保存成一个二进制数组，所以Bitmap也可以看做是bit数组。Bitmap也可以按位做与/或/异或操作

### 基数统计

Set和Hash都可以，但当一个key对应的Set和Hash非常大时，就会导致占用较大内存，成为big key。Redis还提供HyperLogLog，其固定占用12KB内存，但可以计算接近2^64个元素的基数，但HyperLogLog统计是基于概率的，标准误差为0.81%，如果需要精确统计，只能使用Set或Hash

### LBS(Location-Based Service)基于位置信息服务

GEO数据结构。LBS应用访问的数据是人或物关联的一组经纬度信息，且要查询相邻的经纬度范围，比如叫车是查询附近的车

LBS场景下Hash无法满足范围查找，Sorted Set权重是float，而经纬度是两个值，无法将两个值保存为一个float，因此就要用到GEO类型的GeoHash编码

GeoHash编码基本原理就是：二分区间，区间编码

在对经纬度进行GeoHash编码时，首先对经度和纬度分别进行编码，然后再把经纬度合成一个最终的编码

对于地理位置而言，经度范围`[-180, 180]`，GeoHash对一个经度值进行N次二分区操作，比如先分为左区`[-180, 0]`和右区`[0, 180]`，如果经度在左区就记0反之记1，经过N次二分区操作得到N位的二进制数；纬度范围为`[-90, 90]`，采用同样二分区操作得到编码值，最后将经纬度的编码值按奇偶位生成最终编码(比如经度编码值abc，纬度编码值xyz，则最终编码值axbycz)

GEO数据结构底层也是Sorted Set结构，其将最终的编码值作为Sorted Set权重值

使用GeoHash编码后相当于将整个地理空间划分为一个各方格，每个方格覆盖了一定范围的经纬度，分区越多(二分区操作N越大)，每个方格覆盖到的地理空间就越小，也就越精准，当把所有方格编码值映射到一维空间时，相邻的方格的编码值相近，且表示的实际经纬度是相邻的

![GeoHash方格](./pics/geohash_graph.jpg)

上图中可以看到相邻的编码的方格代表的实际经纬度也是相邻的，因此在查询时直接查询所在经纬度编码的邻近的编码处即可找到实际相邻的记录，但注意并不是所有编码值相邻的在实际空间上也是相邻的，比如0111和1000这两个方格，因此在实际查询中可以同时查询所在经纬度周围的4或8个方格，以避免该问题

### 点查询 + 范围查询 + 聚合计算

比如一个设备每秒都会上报自身信息，我们在查询时需要能够查出指定某一秒的信息(点查询)，某一时间范围的信息(范围查询)，某一时间范围内的聚合信息(聚合计算)

可以通过同时写入Hash和Sorted Set实现点查询和范围查询：

以每秒作为Hash中的key，从而实现根据每秒在O(1)时间内查到对应信息

以每秒作为Sorted Set的Score，从而实现在O(1)时间内查出指定秒数范围内的信息

当Hash插入相同key时是会覆盖掉旧值(某一秒的数据更新了两次)，但Sorted Set中不会覆盖而是新增一条记录(某一秒的数据记录了两条)，这点需要尤其注意，因为可能造成数据不一致(即使保证了原子性)

注意：Hash + Sorted Set方式下聚合计算只能由Redis将所有数据传回客户端自行计算，当数据量大时会造成网络IO压力，如果特别关注聚合计算，可以使用RedisTimeSeries模块，这是专门为存取时间序列数据而设计的扩展模块

最后在写入数据时通过Redis的事务来保证写入的原子性，即保证要么Hash和Sorted Set都写入了，要么Hash和Sorted Set都没写入

通过MULTI命令表示一系列原子性操作开始，收到这个命令后Redis会将接下来收到的命令都放到一个内部队列中，后续一起执行；通过EXEC命令表示一系列原子性操作结束，当Redis收到该命令后会开始执行前面放到内部队列中的所有命令。在使用MULTI和EXEC时客户端建议使用pipeline，这样客户端会将命令一次性发给服务端，从而减少客户端和服务端来回网络IO次数，提升性能

### 充当消息队列

消息队列需要保证三个需求：消息保序，处理重复消息和消息可靠性

通过Redis的List结构实现：

List本身的数据结构保证了有序性，生产者通过LPUSH写入List，消费者通过RPOP读取消息，因为生产者生产消息后是无法通知到消费者的，而RPOP又是立刻返回的，所以消费者就必须死循环调用RPOP来查看是否有新消息，这是非常消耗性能的。Redis对此提供了BRPOP阻塞式读取，消费者在没有读取到队列数据时自动阻塞，直到有新数据写入队列再返回给消费者，这样节省CPU开销

List本身无法保证重复消息的处理，因此需要自行为每个信息提供一个唯一ID，消费者将已消费过的ID记录下来以避免重复消费消息

List本身无法保证可靠性，当消费者从List中取出一条消息后List就不会再保存该消息，如果消费者消费失败就代表这条消息被丢失了，因此Redis提供BRPOPLPUSH命令，当消费者读取消息后会先从本来的List中删除，再写入备份List中，当消费者宕机恢复后可以到备份List中重新处理。注意即使清理备份List中的数据，否则备份List会存储过多无用的数据浪费内存

通过Redis的Stream结构实现：

Stream本身保证了有序性，并提供了block配置，实现类似BRPOP阻塞读取的功能并且支持指定阻塞时间，此外其本身还会自动生成一个唯一ID以保证重复消息处理的问题，Stream通过ACK机制保证消息可靠性，即消费者消费了该消息后需要主动给Redis发送XACK命令以表示该消息成功被消费，否则消费者在重启后依旧可以继续处理未被ACK的消息

Stream结构相较于List最大的区别在于提供了消费组的概念，在List中笼统来说只有一个消费组，所有消费者去消费List中消息，一旦一个消息被一个消费者消费后其他消费者都消费不到，又因为只有一个消费组或者说没有消费组的概念，不可能再被其他消费者消费了，而Stream中提供了消费组的概念，同一消费组下的消费者消费过的消息是会被同消费组下其他消费者消费的，但消费组和消费组之间是隔离的，在被的消费组中这些消息还是可以被再次消费的(非常类似于Kafka，并且实现机制也类似于offset机制)

![消息队列List和Stream总结](./pics/mq_list_stream.jpg)

总结：Redis虽然能够实现消息队列的能力，但存在消息丢失的可能，其一是如果AOF是每秒落盘的话，Redis宕机时会丢失这个一秒的数据，如果想保证就必须AOF同步落盘，性能就会下降，其二，当采用主从集群时，如果写入量比较大时会存在主从延迟的问题，此时如果进行主从切换也会存在数据丢失的问题(主库还没同步到从库的数据)，相较于专业的消息队列Kafka，在集群写入操作时采用多个节点+预写磁盘的方式以保证数据一致性，即使主节点挂了也能保证集群中数据不丢失

个人理解：用Redis实现消息队列也会造成big key的问题，因为所有的消息数据都被放在同一key中，无法利用到切片集群的优势，也会导致内存分配不均匀的问题
